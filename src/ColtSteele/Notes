Big O Notation
Double for loop means O(n^2) quadratic
Single loop means O(n) linear
A single variable assignment, array index lookup,
or hashmap look up, is constant time O(1)
If you have two for loops directly after each other
you know its just O(n) + O(n) = 2*O(n)
Since we get rid of constants we can say it
just simplifies down to O(n)

O(n)
function logAtMost10(n) {
    for (var i = 1; i <= Math.min(n, 10); i++) {
        console.log(i);
    }
}
O(n^2)
function subtotals(array) {
    var subtotalArray = Array(array.length);
    for (var i = 0; i < array.length; i++) {
        var subtotal = 0;
        for (var j = 0; j <= i; j++) {
            subtotal += array[j];
        }
        subtotalArray[i] = subtotal;
    }
    return subtotalArray;
}


Rule of thumb:
Constants don't matter
Smaller terms don't matter
The bigger term is the only thing
that matters

Logarithmic Complexity:
Is the next best thing to constant complexity

Palindrome - a sequence of characters that is the
same backwards and forwards

Anagram - a word or phrase that is formed
by rearranging another word or phrase
Example - FRIED = FIRED

Problem Solving Patterns

Frequency Counters:
Involve look up style data structures like
HashMaps & Arrays.
Help get rid of nested for loops

Multiple Pointer:
Its exactly what you think it is.
Multiple pointers going through some
sort of iterable data structure

Helper method recursion is when you don't do all the work in
a single function. You end up having two functions.
The first function is there to set stuff up, and collect the data
The second function is there to do the actual work and heavy lifting
Usually it end up being that the first function has some sort of array
that is added to by the second function, and then returned at the end
as the final answer.

function solveP(int input){
    ArrayList<Integer> results = new ArrayList<>();
    do_some_work(results, someStartVal);
    return results;
}

function do_some_work(ArrayList<Integer> input, int val){
    do stuff here
}


pure recursion is just purely a function calling its self and
eventually returning the right answer

like CS_11_2

Big O (Worse Case) of Linked List & Doubly Linked List is:
Insertion is O(1)
Search / Access / Remove is O(n) - finding the second last thing

Big O of Stack & Queues:
Insertion is O(1)
Search is O(n)

O(log(n)) and O(1) pretty much the same thing
Best you can do
Priority QQ
all worst cases
Get max -> O(1)
Remove max -> O(log(n)) - you are re-balancing
that involves reducing your problem space by half each time
Insert value -> O(log(n)) - you are re-balancing
that involves reducing your problem space by half each time
16 elements in the list means 4 compares
log(16) base 2 is 4
every time you double nodes, you increase compares by 1
Search -> O(n) - just looking through the list of nodes
Heaps can never look like a long line, cause you force them to take
left and right children and then keep adding below

Hash Tables:
- Basically a key value store
- backed by an array that doubles every time you run out of space
1) you hash your key, and use that as an index into your array
2) if you have a hash collision you use a linked list to add depth to the
entry
- Keep things here is that you need to make sure your hashing function
is really good such that it doesn't take too long to run.
and that it does it well enough that you don't get that many collisions

Graphs:
